<p>
تمرین 1: در این تمرین روش  kfold cross-validation بر روی دیتاست mnist اجرا شد. بدون cv، برای کلاس “true” (یعنی درستی پیشبینی عدد 5 )، f1-score 0.97 است. با اعمال cv (استراتیفاید) برای شرایط مشابه f1-score بازهم 0.97 بدست آمد که عملکرد مدل را تایید میکند.
</p>
<p>

تمرین 2: در این تمرین برای optimizer های مختلف شامل SGD، RMSProb، Adam، AdamW و Nadam عملکرد مدل شبکه عصبی شامل دولایه dense  64 تایی relu و نهایی با یک نورون sigmoid بررسی شد. بهترین عملکرد مربوط به بهینه ساز adam با f1-score بیش از 0.88 بوده است.
</p>
<p>
تمرین 3: در این تمرین روش cross-validation با k=5 به مدل قبلی اضافه شد. به جای بهینه سازهای مختلف، توابع فعال ساز مختلف شامل relu، sigmoid و tanh برای لایه dense در نظر گرفته شد. بهترین علمکرد بدست آمده مربوط به تابع عملکرد relu بوده است.
</p>
<p>
  هر سه تمرین در قالب یک فایل ipynb ارایه شده است.
</p>

